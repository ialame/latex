\documentclass{beamer}
\usepackage[francais]{babel}
\usepackage[utf8]{inputenc} % Required for including letters with accents
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{pythontex}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{stmaryrd}
\usepackage{tikz} 
\usetikzlibrary{matrix,decorations.pathreplacing,calc,fit,backgrounds}
\usetikzlibrary{patterns}
%\usetikzlibrary{intersections}
\usepackage {mathtools} 
%%%%%%%%%%%%%%%

%\usepackage[latin1]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage[francais]{babel}
\usepackage[cache=false]{minted}


%%%%%%%%%%%%%%%


\usepackage{stmaryrd}
%\usepackage{tikz}
%\usetikzlibrary{tikzmark}
\usepackage{empheq}
\usepackage{longtable}
\usepackage{booktabs} 
\usepackage{array}
\usepackage{pstricks}
\usepackage{pst-3dplot}
\usepackage{pst-tree}
\usepackage{pstricks-add}
\usepackage{upgreek}
%\usepackage{epstopdf}
\usepackage{eolgrab}
\usepackage{chngpage}
 \usepackage{calrsfs}
 % Appel du package pythontex 
\usepackage{pythontex}

\usetikzlibrary{decorations.pathmorphing}
\def \de {{\rm d}}
\usepackage{color}
%\usepackage{xcolor}
%\usepackage{textcomp}
\newcommand{\mybox}[1]{\fbox{$\displaystyle#1$}}
\newcommand{\myredbox}[1]{\fcolorbox{red}{white}{$\displaystyle#1$}}
\newcommand{\mydoublebox}[1]{\fbox{\fbox{$\displaystyle#1$}}}
\newcommand{\myreddoublebox}[1]{\fcolorbox{red}{white}{\fcolorbox{red}{white}{$\displaystyle#1$}}}
\usetheme[options]{Boadilla}
\definecolor{purple2}{RGB}{153,0,153} % there's actually no standard purple
\definecolor{green2}{RGB}{0,153,0} % a darker green
\usepackage{xcolor}
%\setbeamercolor{background canvas}{bg=lightgray}
\usepackage{listings}
\definecolor{purple2}{RGB}{153,0,153} % there’s actually no standard purple
\definecolor{green2}{RGB}{0,153,0} % a
\lstset{%
language=Python, % 
basicstyle=\normalsize\ttfamily, % 
% Color settings to match IDLE style 
keywordstyle=\color{orange}, % 
keywordstyle={[2]\color{purple2}}, % 
stringstyle=\color{green2}, 
commentstyle=\color{red}, 
upquote=true, %
}
\lstdefinestyle{Python}{
    language        = Python,
    basicstyle      = \ttfamily,
    keywordstyle    = \color{blue},
    keywordstyle    = [2] \color{teal}, % just to check that it works
    stringstyle     = \color{violet},
    commentstyle    = \color{red}\ttfamily
}
\usepackage{algorithm2e}
\RestyleAlgo{algoruled}
  \SetKw{KwFrom}{from} 
\newenvironment{algo}{
\begin{algorithm}[H]
\DontPrintSemicolon \SetAlgoVlined}
{\end{algorithm}}

\usepackage{varwidth}

\usepackage{etex} 
\usepackage{easybmat} 
\usepackage{lmodern} 


%\tikzset{% 
%  highlight/.style={rectangle,rounded corners,fill=orange!35,draw,thick,inner sep=2pt} 
%} 
%\newcommand{\tikzmark}[2]{\tikz[remember picture,baseline=(#1.base),inner sep=0,outer sep=0pt] \node (#1) {#2};} 



  \title{Analyse numérique matricielle}
  \author{ \textsc{Ibrahim ALAME}}\institute{ESTP}
\date{01/03/2023}
  \begin{document}
  \lstset{
    frame       = single,
    numbers     = left,
    showspaces  = false,
    showstringspaces    = false,
    captionpos  = t,
    caption     = \lstname
}
\pgfmathsetmacro{\myscale}{2}
\pgfkeys{tikz/mymatrixenv/.style={decoration={brace},every left delimiter/.style={xshift=8pt},every right delimiter/.style={xshift=-8pt}}}
\pgfkeys{tikz/mymatrix/.style={matrix of math nodes,nodes in empty cells,
left delimiter={[},right delimiter={]},inner sep=1pt,outer sep=1.5pt,
column sep=8pt,row sep=8pt,nodes={minimum width=20pt,minimum height=10pt,
anchor=center,inner sep=0pt,outer sep=0pt,scale=\myscale,transform shape}}}
\pgfkeys{tikz/mymatrixbrace/.style={decorate,thick}}

\newcommand*\mymatrixbraceright[4][m]{
    \draw[mymatrixbrace] (#1.west|-#1-#3-1.south west) -- node[left=2pt] {#4} (#1.west|-#1-#2-1.north west);
}
\newcommand*\mymatrixbraceleft[4][m]{
    \draw[mymatrixbrace] (#1.east|-#1-#2-1.north east) -- node[right=2pt] {#4} (#1.east|-#1-#2-1.south east);
}
\newcommand*\mymatrixbracetop[4][m]{
    \draw[mymatrixbrace] (#1.north-|#1-1-#2.north west) -- node[above=2pt] {#4} (#1.north-|#1-1-#3.north east);
}
\newcommand*\mymatrixbracebottom[4][m]{
    \draw[mymatrixbrace] (#1.south-|#1-1-#2.north east) -- node[below=2pt] {#4} (#1.south-|#1-1-#3.north west);
}


\tikzset{greenish/.style={
    fill=green!50!lime!60,draw opacity=0.4,
    draw=green!50!lime!60,fill opacity=0.1,
  },
  cyanish/.style={
    fill=cyan!90!blue!60, draw opacity=0.4,
    draw=blue!70!cyan!30,fill opacity=0.1,
  },
  orangeish/.style={
    fill=orange!90, draw opacity=0.8,
    draw=orange!90, fill opacity=0.3,
  },
  brownish/.style={
    fill=brown!70!orange!40, draw opacity=0.4,
    draw=brown, fill opacity=0.3,
  },
  purpleish/.style={
    fill=violet!90!pink!20, draw opacity=0.5,
    draw=violet, fill opacity=0.3,    
  }}



 \begin{frame}
 \begin{center}
 Chapitre 3
 \end{center}
  \titlepage
  \end{frame}

\begin{frame}
\frametitle{Système triangulaire}

Un système triangulaire supérieur est un système dont la matrice est triangulaire supérieure.
\begin{empheq}[left=\empheqlbrace]{align*}
 a_{11} x_1 + a_{1 1} x_2 + \dots +a_{1 n} x_n & = b_1 \\
 a_{22} x_2 + \dots + a_{2n} x_n &= b_2 \\
 & \; \vdots \\
  a_{nn} x_n &= b_n
\end{empheq}

Si  $a_{ii}$ non nuls, le système admet une solution unique. 

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
import random as r
import numpy as np

def triangSup(n):
    M=np.zeros((n,n),dtype=int)
    for i in range(n):
        for j in range(i,n):
            M[i,j]=r.randint(1,10)
    return M
\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Système triangulaire}
La résolution  se fait par substitution arrière :
\begin{empheq}[left=\empheqlbrace]{align*}
 x_n &= \frac{b_n}{a_{nn}} \\
 x_k &= \frac{1}{a_{kk}} \left( b_k - \sum_{j=k+1}^n a_{kj} x_j \right)\qquad \forall k \in \llbracket n-1, \dots, 1 \rrbracket
\end{empheq}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Système triangulaire}

\begin{algo}\label{algotrigsup}
\caption{Système triangulaire supérieur}
\For{$k$ \KwFrom $n-1$ \KwTo $1$}{
$x_k=b_k$\;
\For{$j$ \KwFrom $k+1$ \KwTo $n$}{
$x_k=x_k -a_{kj} x_j$\;
}
$x_k=\displaystyle \frac{x_k}{a_{kk}}$
}
\end{algo}

Le calcul de $x_k$ nécessite $n-k$ flops\footnote{\textit{FLOPS} est l'acronyme anglais de \textit{FLoating point Operations Per Second} signifiant \textit{opérations à virgule flottante par seconde}. Il s'agit de la vitesse de calcul d'une opération élémentaire (addition, multiplication) en informatique.} et une division. Le coût de l'algorithme est donc $1+2+ \dots + n-1 = \frac{n(n-1)}{2}$ flops et $n$ divisions.
\end{frame}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
n=5
A=triangSup(n)
x=np.arange(n)
b=np.dot(A,x)

#  Resolution du système traiangulaire supérieur Ax=b
def sysTriangSup(A,b):
    n=len(A)
    x=np.zeros(n,dtype=float)
    for k in range(n-1,-1,-1):
        x[k]=(b[k]-np.dot(A[k,k+1:],x[k+1:]))/A[k,k]
    return x
    
print(sysTriangSup(A,b))


\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Système triangulaire inférieur}
Dans le cas d'un système triangulaire inférieur, l'algorithme est analogue et de même coût :
\begin{empheq}[left=\empheqlbrace]{align*}
 x_1 &= \frac{b_1}{a_{11}} \\
 x_k &= \frac{1}{a_{kk}} \left( b_k - \sum_{j=1}^{k-1} a_{kj} x_j \right)\qquad \forall k \in \llbracket 2, n \rrbracket
\end{empheq}

\begin{algo}
\caption{Système triangulaire inférieur}
\For{$k$ \KwFrom  $1$ \KwTo $n$}{
$x_k=b_k$\;
\For{$j$ \KwFrom $1$ \KwTo $k-1$}{
$x_k=x_k -a_{kj} x_j$\;
}
$x_k=\frac{x_k}{a_{kk}}$
}
\end{algo}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
def triangInf(n):
    M=np.zeros((n,n),dtype=float)
    for i in range(n):
        for j in range(i+1):
            M[i,j]=r.randint(1,10)
    return M
#  Resolution du système traiangulaire inférieur Ax=b
def sysTriangInf(A,b):
    n=len(A)
    x=np.zeros(n,dtype=float)
    for k in range(n):
        x[k]=(b[k]-np.dot(A[k,:k],x[:k]))/A[k,k]
    return x    

n=5
A=triangInf(n)
xp=np.arange(1,n+1)
b=np.dot(A,xp)
print(sysTriangInf(A,b))

\end{minted}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Méthode de Gauss}

On pose $A^{(1)}=A$, $b^{(1)}=b$ et on note $a_{ij}^{(1)}=a_{ij}$ et  $b_{ij}^{(1)}=b_{ij}$ les coefficients de $A^{(1)}$ et $B^{(1)}$.

%%%

On suppose que pour un $k$ fixé on a $A^{(k)}=$ 


\end{frame}

\begin{frame}[fragile,shrink=40]
\[
%    \mathbf{X} = 
    \begin{tikzpicture}[baseline={-0.5ex},mymatrixenv,scale=0.7]
        \matrix [mymatrix,inner sep=4pt] (m)  
        {
    a_{1\,1}^{(k)} & a_{1\,2}^{(k)} & \cdots &   &  & & \cdots &   a_{1\,n}^{(k)}   \\
0 & a_{2\,2}^{(k)} &  &  &  & & & a_{2\,n}^{(k)}  \\
 \vdots &  0 & \ddots &  &  &   &&  \vdots \\
 &   \vdots  &  & a_{k-1\, k-1}^{(k)} &    & & &  \\
 &  &  & 0& a_{k\, k}^{(k)}  &    & &  \\
 &  &  \cdots &\vdots  & a_{k+1\, k}^{(k)} &  a_{k+1\, k+1}^{(k)}  & &  \\
 & &   &   & \vdots & \vdots &\ddots &  \vdots \\
0& 0 &  & 0 & a_{n\, k}^{(k)} &  a_{n\, k+1}^{(k)}  & & a_{n\,n}^{(k)}  \\    
    };

    \begin{scope}[on background layer,rounded corners]
     \node [fit=(m-2-1) (m-8-1),purpleish,inner xsep=0.5pt,inner ysep=3.5pt]{};
     \node [fit=(m-3-2) (m-8-2),purpleish,inner xsep=0.5pt,inner ysep=3.5pt]{};
     \node [fit=(m-5-4) (m-8-4),purpleish,inner xsep=0.5pt,inner ysep=3.5pt]{};
      \node [fit=(m-5-5) (m-5-5),orangeish,inner xsep=0.5pt,inner ysep=3.5pt]{};
      \node [fit=(m-6-5) (m-8-5),cyanish,inner xsep=0.5pt,inner ysep=3.5pt]{};
%     \node [fit=(m-1-3) (m-4-3),purpleish,inner xsep=0.5pt,inner ysep=3.5pt]{};
%     \node [fit=(m-3-1) (m-3-4),brownish,inner xsep=0.5pt,inner ysep=1.5pt]{};
%     \node [fit=(m-3-3) (m-4-4),orangeish]{};
%     \node [fit=(m-3-4) (m-6-5),purpleish,inner xsep=0.5pt,inner ysep=3.5pt,yshift=1pt]{};
%     \node [fit=(m-4-3) (m-4-6),brownish]{};
%     \node [fit=(m-4-4) (m-6-6),cyanish,inner xsep=1.5pt,inner ysep=0.5pt,xshift=-1pt]{};
    \end{scope}

\end{tikzpicture}
\]
\end{frame}





\begin{frame}
Les coefficients de la matrice $A^{(k+1)}$ sont déterminées par récurrence:
\begin{itemize}
\item Les coefficients dans les lignes $L_i$, $1 \leqslant i \leqslant k$ ne sont pas modifiés.
\item Pour $i \geqslant k+1$, si $a_{kk}^{(k)} \neq 0$ :
\begin{empheq}[left=\empheqlbrace]{align*} 
a_{ij}^{(k+1)} &= a_{ij}^{(k)} - \frac{a_{ik}^{(k)}}{a_{kk}^{(k)}} a_{kj}^{(k)} \qquad \forall i \in  \llbracket k+1, n \rrbracket \\
b_{i}^{(k+1)} &= b_{i}^{(k)} - \frac{a_{ik}^{(k)}}{a_{kk}^{(k)}} b_{k}^{(k)}
\end{empheq}
\end{itemize}
On obtient alors un système équivalent,  triangulaire $A^{(n)} \cdot x =b^{(n)}$ . 


\begin{algo}
\caption{Méthode du pivot de Gauss}
\For{$k$ \KwFrom $1$ \KwTo $n-1$}{
\For{$i$ \KwFrom $k+1$ \KwTo $n$}{
$c = \frac{a_{ik}}{a_{kk}}$\;
$b_i = b_i - c  b_k$\;
$a_{ik}=0$\;
\For{$j$ \KwFrom $k+1$ \KwTo $n$}{
$a_{ij}=a_{ij}-c  a_{kk}$}
}
}
\end{algo}
\end{frame}


\begin{frame}
Le coût de la méthode de Gauss est de l'ordre de $\frac{2}{3} n^3$.

À présent, le système est triangulaire supérieur. Nous appliquons l'algorithme \ref{algotrigsup} pour achever la résolution.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
def ReductionGauss(A,B):
    n=A.shape[0]
    U=np.concatenate((A,B),axis=1)
    #U.dtype=np.float64
    for j in range(n):
        for i in range(j+1,n):
            r=U[i,j]/U[j,j]
            U[i]=U[i]-r*U[j]
    return U

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Factorisation LU}

L'opération algébrique $L_i \gets L_i + \lambda L_j$  se traduit matriciellement par la multiplication à gauche  par la matrice $P = {\rm I} + \lambda E_{ij} $ :
\[P=\left(
\begin{BMAT}(c){ccccccc}{ccccccc}
1 & 0 &  \cdots & && \cdots& 0 \\
0 & \ddots & 0 &&& & \vdots\\
\vdots & & &&& &  \\
&&&&&&\\
 && \lambda &&&& \\
 && &&& \ddots& 0\\
0&\cdots& &&&0& 1 
\end{BMAT} \right)
\]

A l'étape $k$ de la méthode de Gauss, l'annulation des coefficients sous la diagonale de la colonne $k$ est équivalent à la multiplication par la matrice $P_k=\prod_{i=k+1}^n (I_d + \lambda_{ik})=I_d +\sum\limits_{i=k+1}^n \lambda_{ik} E_{ik}$.
Cette matrice est inversible d'inverse  :
\[
P_k^{-1}=I_d -\sum\limits_{i=k+1}^n \lambda_{ik} E_{ik}
\]








\end{frame}


\begin{frame}
\frametitle{Factorisation LU}


La multiplication $P \cdot A$ ajoute à la ligne $i$ $\lambda$ fois la ligne $j$. Cette matrice est inversible et le produit à droite par $P^{-1} = {\rm I} - \lambda \cdot E_{ij}$  ajoute à la $j$ème colonne $-\lambda$ fois la colonne $i$. Les opérations linéaires simples appliquées à $A$ afin d'obtenir une matrice triangulaire supérieure se traduisent par la relation :
\[
P_{n-1} \cdots P_2 \cdot P_1 \cdot A =U 
\]
On a :
\[
A=(P_1^{-1} P_2^{-1}  \cdots P_{n-1}^{-1}) \cdot U
\]
Le produit $L = P_1^{-1} \cdot P_2^{-1}  \cdots P_l^{-1}$ est une matrice triangulaire inférieure dont les coefficients diagonaux sont tous égaux à $1$. 

\end{frame}


\begin{frame}
\frametitle{Factorisation LU}



\begin{empheq}{align*}
L&= \prod_{k=1}^{n-1} \left( I_d - \sum_{i=k+1}^n \lambda_{ik} E_{ik} \right)^{-1}\\
&= I_d + \sum_{k=1}^{n-1} \sum_{i>k}  \lambda_{ik} E_{ik} \\
&=
\left(\begin{BMAT}(c){ccccc}{ccccc}
1&&&& \\
\lambda_{21} &1&&& \\
\vdots &\lambda_{32}&\ddots&& \\
&\vdots& \ddots &\ddots& \\
\lambda_{n1}&\lambda_{n2}& \cdots &\lambda_{n\, n-1}& 1\\
\end{BMAT}\right)
\end{empheq}

\end{frame}


\begin{frame}
\frametitle{Factorisation LU}

En pratique, les termes non nuls de $U$ et les termes en-dessous de la diagonale de $L$ sont mémorisés dans la matrice $A$.

\begin{algo}
\caption{Factorisation LU}
\For{$k$ \KwFrom $1$ \KwTo $n-1$}{
\For{$i$ \KwFrom $k+1$ \KwTo $n$}{
$a_{ik}= \displaystyle \frac{a_{ik}}{a_{kk}}$\;
\For{$j$ \KwFrom $k+1$ \KwTo $n$}{$a_{ij}=a_{ij}-a_{ik} a_{kj}$
}
}
}
\end{algo}

La résolution du système linéaire consiste simplement à résoudre successivement deux systèmes linéaires triangulaires :
\begin{empheq}[left={A\cdot x=b \Longleftrightarrow L \cdot U \cdot x=b \Longleftrightarrow \empheqlbrace}]{align*}
L \cdot y &=b \\
U \cdot x &=y
\end{empheq}


\end{frame}


\begin{frame}
%\frametitle{Factorisation LU}

Les éléments non nuls de la matrice $L$, après factorisation, ne sont que les coefficients de $A$ sous la diagonale $l_{ij}=a_{ij}$ ($i \leqslant j$).

\begin{algo}
\caption{Résolution après factorisation LU(Première étape)}
\For{$i$ \KwFrom $1$ \KwTo $n$}{
$y_i=b_i$\;
\For{$j$ \KwFrom $k+1$ \KwTo $i-1$}{
$y_i=y_i - a_{ij} y_j$
}
}
\end{algo}

Les éléments non nuls de la matrice $U$ sont stockés dans la matrice $A$ à la même position $u_{ij}=a_{ij}$ ($i \leqslant j$).

\begin{algo}
\caption{Résolution après factorisation LU($2$ème étape)}
\For{$i$ \KwFrom $n-1$ \KwTo $1$}{
$x_i=y_i$\;
\For{$j$ \KwFrom $1$ \KwTo $i-1$}{
$x_i=x_i - a_{ij} x_j$
}
$\displaystyle x_i=\frac{x_i}{a_{ii}}$
}
\end{algo}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
def ReductionGauss(M):
    n=M.shape[0]
    U=M.copy()
    U.dtype=np.float64
    L=np.eye(n,dtype=np.float64)
    for j in range(n):
        for i in range(j+1,n):
            r=U[i,j]/U[j,j]
            L[i,j]=r
            U[i]=U[i]-r*U[j]
    return (L,U)

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Méthode de Cholesky}

Il s'agit d'une méthode directe adaptée au cas d'une matrice $A\in \mathscr{M}_n(\mathbb{R})$ symétrique définie positive $(Ax, x) >0; \forall x \in \mathbb{R}^n$.

Soit la décomposition $L\cdot U$ de $A$ :
\[
A = \widetilde{L} \cdot \widetilde{U}
\]
Avec :
\[
\widetilde{L}=\left(
\begin{BMAT}(c){cccc}{cccc}
1 & 0 &  \cdots& 0 \\
 &  \ddots &   & \vdots\\
 &&  \ddots& 0\\
&& & 1 
\end{BMAT} \right)
\]
Et:
\[
\widetilde{U}=\left(
\begin{BMAT}(c){cccc}{cccc}
u_{11} &  &     &  \\
 0&  u_{22} &   & \\
 \vdots & &\ddots  &  \\
0 & \cdots  & 0 & u_{nn} 
\end{BMAT} \right)
\]

\end{frame}



\begin{frame}

Soit $D$ une matrice diagonale telle que :
\[
\widetilde{U}=\underbrace{\left(
\begin{BMAT}(c){cccc}{cccc}
u_{11} &  0&    \cdots &0  \\
 0&  u_{22} &   & \vdots \\
 \vdots & &\ddots  &  0\\
0 & \cdots  & 0 & u_{nn}
\end{BMAT} \right)}_D
\underbrace{\left(
\begin{BMAT}(c){cccc}{cccc}
\vphantom{u_{11}} 1&  \star &    \cdots & \star  \\
 0&  \vphantom{u_{11}} 1&   &  \vdots \\
 \vdots & &\ddots  & \star  \\
0 & \cdots  & 0 & \vphantom{u_{11}} 1
\end{BMAT} \right)}_{\widetilde{L}^t}
\]
On a :
\begin{empheq}{align*}
A&=\widetilde{L} \cdot D \cdot \widetilde{L}^t \\
&= \left(\widetilde{L} \cdot \sqrt{D} \right) \cdot \left(\sqrt{D} \cdot \widetilde{L}^t \right) \\
A&= L \cdot {L}^t
\end{empheq}
\end{frame}





\begin{frame}
\frametitle{Example}
Soit $A=\left( \begin{BMAT}(c){cc}{cc}
a & b \\
b & c 
\end{BMAT} \right)
$ avec $a>0$ et $ac-b^2>0$.

\begin{empheq}{align*}
A&= L \cdot U \\
&= 
\left( \begin{BMAT}(c){cc}{cc}
1 & 0 \\
\frac ba & 1 
\end{BMAT} \right) 
\left( \begin{BMAT}(c){cc}{cc}
a & b \\
0 & c-\frac{b^2}{a}
\end{BMAT} \right)  \\
&= \left( \begin{BMAT}(c){cc}{cc}
1 & 0 \\
\frac ba & 1 
\end{BMAT} \right)
\left( \begin{BMAT}(c){cc}{cc}
a & 0 \\
0 & c-\frac{b^2}{a}
\end{BMAT} \right)
\left( \begin{BMAT}(c){cc}{cc}
1 & \frac ba  \\
0 & 1
\end{BMAT} \right)
\end{empheq} 

Or $A= L \cdot L^t$ avec $L=
\left( \begin{BMAT}(c){cc}{cc}
1 & 0  \\
\frac ba & 1
\end{BMAT} \right) 
\left( \begin{BMAT}(c){cc}{cc}
\sqrt{a} &0  \\
0 & \frac{\sqrt{ac-b^2}}{\sqrt{a}}
\end{BMAT} \right)$.

Donc :
\[
L = \left( \begin{BMAT}(c){cc}{cc}
\sqrt{a} &0  \\
\frac{b}{\sqrt{a}} & \frac{\sqrt{ac-b^2}}{\sqrt{a}}
\end{BMAT} \right)
\]
%\end{example}

\end{frame}



\begin{frame}
%\frametitle{Example}
\begin{theorem}
Soit $A \in \mathscr{M}_n(\mathbb{R})$ symétrique définie positive. Alors il existe une unique matrice $L \in \mathscr{M}_n(\mathbb{R})$ triangulaire inférieure dont les termes diagonaux sont tous positifs telle que $A= L \cdot L^t$. 
\end{theorem}

Par récurrence sur $n$.
\begin{itemize}
\item Pour $n=1$, $A=(a_{11})$ avec $a_{11} >0$. Donc $L=(l_{11})$ avec $l_{11}=\sqrt{a_{11}}$.
\item Soit $A \in \mathscr{M}_{n+1}(\mathbb{R})$. On écrit $A$ sous la forme :
\[
A= \left( \begin{BMAT}(c){c.c}{c.c}
B & a \\
a^t & \alpha 
\end{BMAT} \right)
\]
$B$ est symétrique et définie positive car $0 <
 \left( A  \cdot  \left( \begin{BMAT}(c){c}{c.c}
y \\
0
\end{BMAT} \right),\left( \begin{BMAT}(c){c}{c.c}
y \\
0
\end{BMAT} \right) \right) = < B \cdot y, y >$.
Par hypothèse de récurrence, il existe une matrice triangulaire inférieure $M$ dont les coefficients diagonaux sont positifs telle que $B=M \cdot M^t$.

On pose $L= \left( \begin{BMAT}(c){c.c}{c.c}
M & 0 \\
b^t & \lambda 
\end{BMAT} \right)$ où $b \in \mathbb{R}^n$ et $\delta>0$.

\end{itemize}

\end{frame}



\begin{frame}



On a :
\[
L \cdot L^t= \left( \begin{BMAT}(c){c.c}{c.c}
M\cdot M^t & M \cdot b \\
b^t \cdot M^t& b^t \cdot b + \lambda^2 
\end{BMAT} \right)
\]

Par identification avec la matrice $A$, on a $M \cdot b=a$ et $b^t \cdot b +\lambda^2=\alpha$. D'où $b=M^{-1} \cdot a$.
et $a^t \cdot M^t \cdot M^{-1} \cdot a + \lambda^2=\alpha$. C'est-à-dire $a^t\cdot B^{-1} \cdot a + \lambda^2=\alpha$. D'où $\lambda^2=\alpha-a^t \cdot B^{-1} \cdot a$.

Le signe de $\alpha- a^t \cdot B^{-1} \cdot a$ est déterminé en choisissant 
$z=
\left( \begin{BMAT}(c){c}{c.c}
B^{-1} \cdot a \\
-1
\end{BMAT}\right)$
dans l'inégalité $A \cdot z \cdot z >0$.



On obtient bien $\alpha-a^t \cdot B^{-1} \cdot a >0$. D'où le choix de $\lambda=\sqrt{\alpha-a^t\cdot B^{-1} \cdot a}$.

Finalement $
L = \left( \begin{BMAT}(c){c.c}{c.c}
M  & 0 \\
 \left(M^{-1} \cdot a \right)^t&  \lambda
\end{BMAT} \right)
$
existe et permet de conclure la récurrence.


\end{frame}



\begin{frame}
\frametitle{Calcul de $L$}

Soit $L =(\ell_{ij})_{ij}$ et $A =(a_{ij})_{ij}$.

$A= L \cdot L^t \Longleftrightarrow \myredbox{a_{ij}=\sum\limits_{k=1}^{\inf(i,j)} \ell_{ik} \ell_{jk} \quad \forall (i, j) \in \llbracket 1, n \rrbracket^2 }$ 

Pour $j=1$ :

\begin{center}
\begin{varwidth}{20cm}
\qquad $a_{i1}=\ell_{i1} \ell_{11}$  %\espacesys 
\\

{\begin{varwidth}{20cm}
\begin{empheq}[left={\Longleftrightarrow \empheqlbrace}]{align*}
&a_{11}=\ell_{11}^2 \\
&a_{i1}=\ell_{i1} \ell_{11}  \quad \forall i \in \llbracket 2, n \rrbracket
\end{empheq}
\end{varwidth}}
\\\\

{\myredbox{\begin{varwidth}{20cm}
\begin{empheq}[left={\Longleftrightarrow \empheqlbrace}]{align*}
&\ell_{11} = \sqrt{a_{11}}\\
&\ell_{i1} =\frac{a_{i1}}{\ell_{11}}  \quad \forall i \in \llbracket 2, n \rrbracket
\end{empheq}
\end{varwidth}}}
\\
\end{varwidth}
\end{center}

\end{frame}

\begin{frame}

On suppose avoir calculé les $j-1$ premières colonnes de $L$. On a alors :
\begin{empheq}{align*}
a_{jj}& = \sum_{k=1}^j \ell_{jk} \ell_{jk} \\
&= \sum_{k=1}^{j-1} \ell_{jk}^2 + \ell_{jj}^2 
\end{empheq}

\[\myredbox{\ell_{jj} = \sqrt{a_{jj} - \sum_{k=1}^{j-1} \ell_{jk}^2}}\]

Et pour $i \in \llbracket j+1, n \rrbracket $ :
\begin{empheq}{align*}
a_{jj}& = \sum_{k=1}^j \ell_{ik} \ell_{jk} = \sum_{k=1}^{j-1} \ell_{ik} \ell_{jk} + \ell_{ij} \ell_{jj} 
\end{empheq}
\[\myredbox{\ell_{ij} = \frac{\displaystyle a_{ij} -  \sum_{k=1}^{j-1} \ell_{ik} \ell_{jk}}{\ell_{jj}} }\]
\end{frame}

\begin{frame}

D'où l'algorithme :
\begin{algo}
\caption{Factorisation de Cholesky}
\For{$j$ \KwFrom $1$ \KwTo $n$}
{
$s=0\;$
\For{$k $ \KwFrom $1$ \KwTo $j-1$}{
$s=s+a_{jk}^2$\;
$a_{jj} = \sqrt{a_{jj} -s}$\;
\For{$i $ \KwFrom $j+1$ \KwTo $n$}{
$s=0$\;
\For{$k $ \KwFrom $1$ \KwTo $j-1$}{
$s=s+a_{ik} a_{jk}$
}
$\displaystyle a_{ij} =\frac{a_{ij}-s}{a_{jj}}$
}
}
}
\end{algo}
Choleski conserve le profile de la matrice.

\end{frame}

\begin{frame}


Coût de calcul de $L$ :
\begin{empheq}{align*}
\sum_{j=1}^n (2j-1) (n-j+1) &= 2 n \sum_{j=1}^n j - 2 \sum_{j=1}^n j^2 + 2 \sum_{j=1}^n j + \sum_{j=1}^n (j-1) \\
&= 2 n \frac{n(n+1)}{2} - 2 \frac{n(n+1)(2n+1)}{6} \\
&= \frac{n^3}{3} + \frac{n^2}{2} + \frac{n}{6}\\
& \simeq \frac{n^3}{3}
\end{empheq}
À titre de comparaison pour $n=10$ :
\begin{itemize}
\item Gauss : $700$ opérations
\item Cholesky : $350$ opérations
\item Cramer : $40\; 000\;000$ opérations
\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
def cholesky(A):
    n=A.shape[0]
    L=np.zeros((n,n),dtype=float)
    for j in range(n):
        s=np.dot(L[j,:j],L[j,:j])
        L[j,j] = np.sqrt(A[j,j]- s)
        for i in range(j+1,n):
            s=np.dot(L[i,:j],L[i,:j])
            L[i,j]=(A[i,j]-s)/L[j,j] 
    return L 

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Normes matricielles}
\begin{block}{Norme}
On appelle norme matricielle sur $\mathscr{M}_n \left( \mathbb{R} \right)$ induite par la norme vectorielle de $\mathbb{R}^n$ l'application:
\[
A \longmapsto \| A \| = \sup_{\| x\| =1} \| A x \|
\]
\end{block}

\begin{block}{proposition}
\begin{itemize}
\item $\| A x \| \leqslant \| A \| \cdot \| x \| $
\item  $\| A  \| = \max\limits_{x \neq 0} \frac{\| A x \|}{\| x \|}$
\item $\| A B \| \leqslant \| A \| \cdot \| B \|$
\end{itemize}
\end{block}

Examples:
\[ \| A \| _\infty = \max\limits_i \sum\limits_{i, j} \| a_{ij} \| \qquad  \|A \|_1 = \max\limits_{j} \sum\limits_{i, j} | a_{ij} |\qquad    \|A \|_2= \sqrt{\rho( A \cdot A^t)}\]

Si $A$ est symétrique $\|A \|_2 = \rho(A)$. 


\end{frame}

\begin{frame}
\frametitle{Normes matricielles}
\begin{block}{proposition}
Soient $A \in \mathscr{M}_n ( \mathbb{R})$ et $ \varepsilon>0$. Il existe une norme induite sur $\mathscr{M}_n(\mathbb{R})$ notée $\| \bullet \|_{A, \varepsilon}$ qui vérifie $\| A \|_{A, \varepsilon}  \leqslant \rho(A) + \varepsilon$.
\end{block}
\begin{block}{corollaire}
On munit $\mathscr{M}_n (\mathbb{R})$ d'une norme $\| \cdot \|$. Soit $A \in \mathscr{M}_n (R)$. Alors $\rho(A)<1$ si et seulement si $\lim\limits_{k \to \infty} A^k=0$. 
\end{block}

%\begin{demo}
Si $\rho(A) < 1$ alors  $ \exists \varepsilon>0$ tel que $\rho(A) < 1 - 2 \varepsilon$. 
\begin{itemize}
\item $ \| A \|_{A, \varepsilon} \leqslant \rho + \varepsilon < 1 - \varepsilon <1 $
\item $ \| A^k \|_{A, \varepsilon} \leqslant \| A \|_{A, \varepsilon}^k < (1 - \varepsilon)^k \to 0$
\end{itemize}
Donc $ \lim\limits_{k \to \infty } \| A^k \| = 0$ en dimension finie.

Réciproquement, soit $\lambda$ une valeur propre de $A$. On a $A^kx= \lambda^k x$. Donc si $\lim\limits_{k \to \infty} A^k=0$,  alors $\lim\limits_{k \to \infty }\lambda^k x =0$ et donc $| \lambda | <1$.
%\end{demo}

\end{frame}
\begin{frame}
\begin{block}{proposition}
Soit $A \in \mathscr{M}_n ( \mathbb{R})$ muni d'une norme matricielle $\| \cdot \|$. Alors :
\[
\rho(A) \leqslant \| A \|
\]
\end{block}
%Démo: $\| A^k \| < \| A \|^k$

\begin{block}{corollaire}
Pour montrer que la suite $x^{(k+1)} = A \cdot x^{(k)}$  converge, il suffit de trouver une norme matricielle telle que $\| A \| <1$.
\end{block}

\begin{theorem}
Si $\| A \| <1$, alors ${\rm I} + A $ inversible et on a :
\[
\left\| (I+A)^{-1} \right\| \leqslant \frac{1}{1-\|A\|}
\]
Si ${\rm I} + A $ est singulière, alors $\| A \| \geqslant 1$ pour toute norme matricielle.
\end{theorem}


\end{frame}


\begin{frame}
\frametitle{Conditionnement}

\begin{block}{definition}
Soit $A \in \mathscr{M}_n(\mathbb{R})$ une matrice inversible. On appelle conditionnement de $A$ par rapport à la norme $\| \bullet \|$ le nombre :
\[
\mbox{cond}(A) = \|A \| \cdot \| A^{-1}\|
\] 
\end{block}

\begin{block}{proposition}
\begin{itemize}
\item $\mbox{cond}(A) \geqslant 1$ car $\| {\rm I} \| \leqslant \|A \| \cdot \| A^{-1}\|$
\item $\mbox{cond}(\lambda A) = \mbox{cond} (A)$
\item $\mbox{cond} (A \cdot B ) \leqslant \mbox{cond}(A) \cdot \mbox{cond}(B)$
\end{itemize}
\end{block}

Exemple: 
Si $A$ symétrique définie positive, alors $\mbox{cond}_2(A)=\frac{\lambda_n}{\lambda_1}$. Sinon, $\mbox{cond}_2(A)=\sqrt{\frac{\sigma_{11}}{\sigma_1}}$ où $\sigma_1 < \cdots <\sigma_n$ valeurs propres de $A \cdot A^t$.



\end{frame}

\begin{frame}

\begin{block}{Propriétés}
\begin{itemize}
\item $\mbox{cond}_2(A) =1 $ si et seulement si $A = \alpha Q$ avec $Q $ orthogonale
\item  Si $A= Q \cdot R$, alors  $\mbox{cond}_2(A)= \mbox{cond}_2(R)$.
\item Si $A$ et $B$ sont symétriques définies positives, alors $\mbox{cond}_2(A+B) \leq \max\left\{ \mbox{cond}_2(A), \mbox{cond}_2(B)\right\}$
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\begin{block}{proposition}
Si $x$ solution de $A x=b$ et $x + \delta x$ solution de $A(x + \delta x)=b+ \delta b$, alors :
\[
\frac{\| \delta x \|}{\| x \|} \leqslant \mbox{cond}(A) \cdot \frac{\| \delta b\|}{\| b \|}
\]
\end{block}

Démo:

Nous avons $A \cdot x =b$ et $A (x+\delta x)=b+\delta b $. Donc $A \cdot \delta x = \delta b$. $A $ est inversible, donc $\delta x= A^{-1} \cdot \delta b$. Par passage à la norme: $\myredbox{\| \delta x \| \leqslant \| A^{-1} \| \cdot \| \delta b \|}$ . 

D'autre part, $b=A\cdot x$, donc $\| b \| \leqslant \|A\| \cdot\|x\|$. Donc $\myredbox{\frac{1}{\| x \|} \leqslant \frac{\| A \|}{\| b \|}}$. 

Par multiplication des deux inégalités, nous obtenons $\frac{\| \delta x \|}{\| x \|} \leqslant \| A\|^{-1} \cdot \| A \| \cdot \frac{\| \delta b\|}{\| b \|}$. 


\end{frame}

\begin{frame}
\begin{block}{proposition}
Si $x$ solution de $A \cdot x=b$ et $x + \delta x$ solution de $(A + \delta A)(x + \delta x)=b$, alors :
\[
\frac{\| \delta x \|}{\| x + \delta x\|} \leqslant \mbox{cond}(A) \cdot \frac{\| \delta A\|}{\| A \|}
\]
\end{block}

Démo:

On a $A\cdot x =b$ et $(A+ \delta A)(x+ \delta x)=b$. Donc $A \delta x = - \delta A(x + \delta x)$. Par passage à la norme, nous obtenons :
\begin{empheq}{align*}
\| \delta x \| &\leqslant \| A^{-1} \| \cdot \| \delta A\|  \cdot \| x + \delta x \| \\
\frac{\| \delta x\|}{\| x + \delta x \| }  &\leqslant \| A^{-1} \| \cdot \| \delta A\|  \\
\frac{\| \delta x\|}{\| x + \delta x \| }  &\leqslant  \| A^{-1} \| \cdot \| A\| \cdot \frac{\| \delta A\|}{\| A \|} \\
\end{empheq}

\end{frame}



\begin{frame}
\begin{theorem}
On suppose $\| \delta A \| < \frac{1}{\| A^{-1}\|}$ et $b \neq 0$. Alors $(A+ \delta A)$ est inversible et si $x$ est solution de $A\cdot x=b$ et $x+ \delta x$ de $(A+ \delta A)(x + \delta x)=b+ \delta b$, on a :
\[
\frac{\| \delta x \|}{\| x \|} \leqslant \frac{\mbox{cond}(A)}{1-\| A^{-1} \| \cdot \| \delta A \|} \left( \frac{\| \delta b \|}{\| b \|} + \frac{\| \delta A \|}{\| A \|}   \right)
\]
\end{theorem}
Démo:

La matrice $I_d +A^{-1} \cdot \delta A$ est inversible car $\rho(A^{-1} \cdot \delta A) \leqslant \|  A^{-1} \cdot \delta A \| \leqslant \| A^{-1} \| \cdot \| \delta A \|$.
On applique alors le théorème. La matrice $A + \delta A= A(I + A^{-1} \cdot \delta A)$ est donc inversible et il existe $\delta x$ tel que :
\[
(A + \delta A) (x + \delta x)=b+ \delta b
\]
Ainsi :
\[
\delta A \cdot x + A \left(I_d + A^{-1} \cdot \delta A \right) \delta x = \delta b
\]
Donc :
\[
\frac{\delta x }{\| x \|} = \left(I_d + A^{-1} \cdot \delta A \right)^{-1} \cdot A^{-1} \left( \frac{ \delta b }{\| x \|} - \frac{ \delta A \cdot x}{\| x \|} \right)
\]

\end{frame}

\begin{frame}
Par passage à la norme :
\begin{empheq}{align*}
  \frac{\| \delta x \|}{\| x \|} & \leqslant \left\| (I_d + A^{-1} \cdot \delta A)^{-1} \right\| \cdot \| A^{-1}\|  \left( \frac{\| \delta b \|}{\| x \|} + \| \delta A \| \right)\\
 &\leqslant \left\| (I_d + A^{-1} \cdot \delta A)^{-1} \right\| \cdot \mbox{cond} (A)  \left( \frac{\| \delta b \|}{\| b \|} + \frac{\| \delta A \|}{\| A \|} \right) \\
\end{empheq}
Par le théorème \ref{thconditionnement} :
\begin{empheq}{align*}
\frac{\| \delta x \|}{\| x \|}&\leqslant\frac{\mbox{cond}(A)}{1 - \| A^{-1} \cdot  \delta A\|}  \left( \frac{\| \delta b \|}{\| b \|} + \frac{\| \delta A \|}{\| A \|} \right) \\
 &\leqslant\frac{\mbox{cond}(A)}{1 - \| A^{-1}\| \cdot \| \delta A\|}  \left( \frac{\| \delta b \|}{\| b \|} + \frac{\| \delta A \|}{\| A \|} \right) 
\end{empheq}

\end{frame}

\begin{frame}
\frametitle{Exemple de système linéaire mal conditionné}
Considérons le système
\[\left(\begin{array}{cccc}
10&7&8&7\\
7&5&6&5\\
8&6&10&9 \\
7&5&9&10
\end{array}\right) \left(\begin{array}{c}
u_1\\
u_2\\
u_3 \\
u_4
\end{array}\right) =\left(\begin{array}{c}
32\\
23\\
33 \\
31
\end{array}\right)  \mbox{ de solution }\left(\begin{array}{c}
1\\
1\\
1 \\
1
\end{array}\right) \]
Le système perturbé
\[\left(\begin{array}{cccc}
10&7&8&7\\
7&5&6&5\\
8&6&10&9 \\
7&5&9&10
\end{array}\right) \left(\begin{array}{c}
u_1+\delta u_1\\
u_2+\delta u_2\\
u_3+\delta u_3 \\
u_4+\delta u_4
\end{array}\right) =\left(\begin{array}{c}
32,1\\
22,9\\
33,1 \\
30,9
\end{array}\right)  \mbox{ de solution }\left(\begin{array}{c}
9,2\\
-12,6\\
4,5 \\
-1,1
\end{array}\right) \]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Exemple de système linéaire mal conditionné}
Pourtant, la matrice est "bonne" (symétrique, de déterminant 1, donc loin de 0). Son inverse est d'ailleurs donnée par
\[A^{-1}=\left(\begin{array}{cccc}
25&-41&10&-6\\
-41&68&-17&10\\
10&-17&5&-3\\
-6&10&-3&2
\end{array}\right) \]
Mais les valeurs propres de $A$ sont
\[\lambda_1 \simeq 0,01015<\lambda_2 \simeq 0,8431<\lambda_3 \simeq 3,858<\lambda_4 \simeq 30,2877\]
\[\mbox{cond}_2(A)=\frac{\lambda_4}{\lambda_1}\simeq 2984 >>1\]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Exemple de système linéaire mal conditionné}
D'autre part
\[u=\left(\begin{array}{c}
1\\
1\\
1\\
1
\end{array}\right) ,\delta u=\left(\begin{array}{c}
8,2\\
-13,6\\
3,5\\
-2,1
\end{array}\right) , b=\left(\begin{array}{c}
32\\
23\\
33 \\
31
\end{array}\right) ,\delta b=\left(\begin{array}{c}
0,1\\
-0,1\\
0,1 \\
-0,1
\end{array}\right)  \]
de sorte que
\[\frac{\|\delta u\|_2}{\| u\|_2}\simeq 8,1985 \mbox{ et } \mbox{cond}_2(A) \frac{\|\delta b\|_2}{\| b\|_2}  \simeq 9,9424\]
On n'est donc pas loin de l'égalité dans l'estimation
\[\frac{\|\delta u\|_2}{\| u\|_2}\leq \mbox{cond}_2(A) \frac{\|\delta b\|_2}{\| b\|_2}  \]
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Méthodes itératives}

\begin{block}{definition}
La méthode itérative construit une suite récurrente $x^{(k)}$ qui converge pour tout $x_0^{(0)}$ vers $x$, solution de $A\cdot x=b$.
\end{block}

On décompose la matrice $A$ sous la forme $A=P-N$. On a alors l'équivalence:
\[
P\cdot x = N \cdot x +b
\]
Si $P$ est inversible, on définit la suite $\left(x^{k} \right)_{k \in \mathbb{N}}$ par:
\begin{empheq}[left=\empheqlbrace]{align*}
&P \cdot x^{(k+1)}=N \cdot x^{(k)} +b \\
& x^{(0)} \in \mathbb{R}^n \text{ donné}
\end{empheq}
Si la suite converge, alors elle converge vers $x=A^{-1}b$.

Dans la suite, on pose $B=P^{(-1)} N$ et $c=P^{-1} \cdot b$ :
%\begin{empheq}[left=\empheqlbrace]{align*}
%&x^{(k+1)}=B \cdot x^{(k)} +c \\
%& x^{(0)} \in \mathbb{R}^n \text{ donné}
%\end{empheq}
\[\myredbox{\left\{\begin{array}{l}
x^{(k+1)}=B \cdot x^{(k)} +c \\
x^{(0)} \in \mathbb{R}^n \text{ donné}
\end{array}\right.}\]
\end{frame}

\begin{frame}

\begin{theorem}
\begin{itemize}
\item La suite $\left( x^{(k)}\right)_{k \in \mathbb{N}}$ converge vers $x=A^{-1} \cdot b$ ssi $\rho(B)<1$.
\item La suite $\left( x^{(k)}\right)_{k \in \mathbb{N}}$ converge vers $x=A^{-1} \cdot  b$ si et seulement si il existe une norme telle que $\|B\|<1$.
\end{itemize}
\end{theorem}

Démo:

Nous avons $x^{(k+1)}-x=B \left(x^{k}- x \right)$. Donc $x^{(k)}-x=B^k  \left(x^{0}- x \right)$. 
\begin{itemize}
\item D'après le corollaire , si $\rho(B)<1$, alors $\lim\limits_{k \to \infty} B^k=0$. D'où la convergence de la suite vers la solution. 
\item Si $\| B \| <1$, alors $\| B^k \| \leqslant \| B \|^k$ et donc $\lim\limits_{k \to \infty} B^k=0$. D'où la convergence de la suite.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{example}
$A=\left(
\begin{BMAT}(c){cc}{cc}
2 & -1 \\
-1 & 2 
\end{BMAT} \right)$, $P={\rm I}$ et $N={\rm I}-A=\left(
\begin{BMAT}(c){cc}{cc}
-1 & 1 \\
1 & -1 
\end{BMAT} \right)=B$.

Les valeurs propres de $B$ sont $0$ et $-2$; $\rho(B)=2$. 

La suite \begin{varwidth}{20cm}
\begin{empheq}[left=\empheqlbrace]{align*}
& x^{(k+1)}=N x^{(k)} +b \\
& x^{(0)} \in \mathbb{R}^n \text{ donné}
\end{empheq} \end{varwidth}
ne converge pas.


Prenons $P=\beta {\rm I}$ et $N=P-A=\beta {\rm I} -A$.

Ainsi :
\begin{empheq}{align*}
B &= I_d - \frac{A}{\beta}\\
&=I_d - \alpha A \\
&=
\left(
\begin{BMAT}(c){cc}{cc}
1-2\alpha & \alpha \\
\alpha & 1-2\alpha 
\end{BMAT} \right)
\end{empheq}
où $\alpha=\frac{1}{\beta}$.
Les valeurs propres de $B$ sont $1-\alpha \lambda$ (où $\lambda$ valeur propre de $A$), $1-\alpha$ et $1-3 \alpha$.
Donc $\rho(B)=\max\{ |1- \alpha|, |1-3 \alpha| \}$

\end{frame}


\begin{frame}
\definecolor{ffqqtt}{rgb}{1,0,0.2}


\begin{center}
\begin{tikzpicture}[line cap=round,line join=round,>=stealth,scale=2,x=2.0cm,y=1.0cm]
\draw[->,color=gray,very thin] (-0.2,0) -- (1.6,0);
\draw[->,color=gray,very thin] (0,-0.2) -- (0,1.2);
\draw (0,1)-- (0.33,0);
\draw (1,0)-- (1.5,0.5);
\draw [dash pattern=on 1pt off 1pt] (0.67,1)-- (0.67,0);
\draw [dash pattern=on 1pt off 1pt,color=red] (0.5,0.5)-- (0.5,0);
\draw [dash pattern=on 1pt off 1pt,color=red] (0,0.5)-- (0.5,0.5);
\draw (0.33,0)-- (0.5,0.5);
\draw (0.5,0.5)-- (1,0);
\draw [color=blue,line width=1.5pt] (0,1)-- (0.5,0.5);
\draw [color=blue,line width=1.5pt] (0.67,1)-- (0.5,0.5);
\fill  (0.67,1) circle (0.5pt);
\fill  (0.5,0.5) circle (0.5pt);
\begin{scriptsize}
\draw (0.5,0) node[below] {\color{red} $ \displaystyle \alpha=\frac{1}{2}$};
\fill  (0,1) circle (0.5pt);
\draw (0,0.5) node[left] {\color{red} $ \displaystyle \rho(B)=\frac{1}{2}$};
\fill  (0,0.5) circle (0.5pt);
\fill  (0.5,0) circle (0.5pt);
\draw (0.0,1.0) node[left] {$1$};
\fill (0.33,0) circle (0.5pt);
\draw(0.33,0) node[below] {$\displaystyle \frac 13$};
\fill (1,0) circle (0.5pt);
\draw(1, 0) node[below] {$1$};
%\fill  (2,0) circle (0.5pt);
%\draw (2, 0) node[below] {$2$};
\fill (0.67,0) circle (0.5pt);
\draw(0.67, 0) node[below] {$\displaystyle \frac 23$};
\end{scriptsize}
\end{tikzpicture}
\end{center}


Méthode converge $0< \alpha<\frac 23$ 

La valeur optimale de $\rho(B)$ correspond à $\alpha=\frac 12$. 


\end{frame}

\begin{frame}
\frametitle{Méthode de Jacobi}

Soit la décomposition  de la matrice $A$ en somme de trois matrices : $A=D-E-F$ où $D$ représente la diagonale de $A$, $-E$ et $-F$ les parties triangulaires inférieures et supérieures.
\[
D=\left(
\begin{BMAT}(c){cccc}{cccc}
a_{11} & 0 &  \cdots& 0 \\
 0 &  \ddots &   & \vdots\\
 \vdots &&  \ddots& 0\\
0 && & a_{nn}
\end{BMAT} \right)\]
\[
-E=\left(
\begin{BMAT}(c){cccc}{cccc}
0& 0 &  \cdots& 0 \\
a_{21}  &  \ddots &   & \vdots\\
 \vdots &&  \ddots& 0\\
a_{n1} && & 0
\end{BMAT} \right);\quad
-F=\left(
\begin{BMAT}(c){cccc}{cccc}
0&  a_{12} &  \cdots& a_{1n}  \\
 \vdots &  \ddots &   & \vdots\\
 \vdots &&  \ddots& 0\\
0&& & 0
\end{BMAT} \right)
\]
\end{frame}

\begin{frame}
La méthode de Jacobi est itérative telle que $P=D$ et $N=E+F$. Elle s'écrit donc :
\begin{empheq}[left=\empheqlbrace]{align*}
&x^{(k+1)}=D^{-1}(E+F)x^{(k)} +D^{-1}b \\
&x_0 \in \mathbb{R}^n
\end{empheq}
On note $B_J=D^{-1}(E+F)$ et $C_J=D^{-1}b$.
\[
B_J=\left(
\begin{BMAT}(c){cccc}{cccc}
0&  -\frac{a_{12}}{a_{11}} &  \cdots &-\frac{a_{1n}}{a_{11}}  \\
-\frac{a_{21}}{a_{22}}   &  \ddots &   & \vdots\\
 \vdots &&  \ddots& 0\\
\frac{a_{n1}}{a_{nn}} && & 0
\end{BMAT} \right)
\qquad
C_J=\left(
\begin{BMAT}(c){c}{cccc}
\frac{b_1}{a_{11}} \\
\frac{b_2}{a_{22}} \\
\vdots \\
\frac{b_n}{a_{nn}} \\
\end{BMAT} \right)
\]

\end{frame}

\begin{frame}

La forme développée de la méthode s'écrit:
\[
\myredbox{x_i^{(k+1)}=\frac{\displaystyle b_i-\sum_{\substack{j=1, j\neq i}}^n a_{ij}x_j^{(k)}}{a_{ii}} \qquad \forall i \in \llbracket 1, n \rrbracket}
\]
où $x^{(0)}$ est un vecteur arbitraire de $\mathbb{R}^n$.

\begin{algo}
\caption{Méthode de Jacobi}
\For{$i$ \KwFrom $1$ \KwTo $n$}{
$s=b_i$\;
\For{$j$ \KwFrom $1$ \KwTo $i-1$}{
$s=s-a_{ij} \cdot x_j$
}
\For{$j$ \KwFrom $i+1$ \KwTo $n$}{
$s=s-a_{ij} \cdot x_j$
}
$y_i=\displaystyle \frac{s}{a_{ii}}$
}
\For{$i$ \KwFrom $i$ \KwTo $n$}{
$x_i=y_i$
}
\end{algo}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
def jacobi(A,b,x0,N):
    x=x0
    for k in range(N):
        n=len(A)
        y=np.zeros(n)
        for i in range(n):
            y[i]=(b[i]-A[i,:i]@x[:i]-A[i,i+1:]@x[i+1:])/A[i,i]
        x=y.copy()
    return x 

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example}
Soit à résoudre le système 
\begin{varwidth}{20cm} \begin{empheq}[left=\empheqlbrace]{align*}
& 10 x_1 + x_2 = 11 \\
& 2 x_1 + 10 x_2 = 12
\end{empheq} \end{varwidth}
dont la solution est $x=\left(\begin{BMAT}(c){c}{cc}1 \\ 1 \end{BMAT}\right)$.


En partant de $x^{(0)}=0$, on obtient successivement :
\[
x^{(1)}=\left(\begin{BMAT}(c){c}{cc} \frac{11}{10} \\ \frac{12}{10} \end{BMAT}\right), \quad
x^{(2)}=\left(\begin{BMAT}(c){c}{cc} \frac{98}{100} \\ \frac{98}{100} \end{BMAT}\right), \quad
x^{(3)}=\left(\begin{BMAT}(c){c}{cc} \frac{1002}{1000} \\ \frac{1004}{1000} \end{BMAT}\right), \quad
x^{(4)}=\left(\begin{BMAT}(c){c}{cc} \frac{9996}{10000} \\ \frac{9992}{10000} \end{BMAT}\right), \quad \dots
\]
La suite de la méthode de Jacobi semble converger vers la solution $x_1=1$, $x_2=1$. 

\end{frame}
\begin{frame}
\frametitle{Example}
Soit à résoudre le système 
\begin{varwidth}{20cm} \begin{empheq}[left=\empheqlbrace]{align*}
&  x_1 +10 x_2 = 11 \\
& 10 x_1 + 2 x_2 = 12
\end{empheq} \end{varwidth}
dont la solution est aussi $x=\left(\begin{BMAT}(c){c}{cc}1 \\ 1 \end{BMAT}\right)$.


En partant également de $x^{(0)}=0$, on obtient  :
\[
x^{(1)}=\left(\begin{BMAT}(c){c}{cc} 11 \\ 6 \end{BMAT}\right), \quad
x^{(2)}=\left(\begin{BMAT}(c){c}{cc} -49 \\ -49 \end{BMAT}\right), \quad
x^{(3)}=\left(\begin{BMAT}(c){c}{cc} 501 \\ 251 \end{BMAT}\right), \quad
x^{(4)}=\left(\begin{BMAT}(c){c}{cc} -2499 \\ -2499 \end{BMAT}\right), \quad \dots
\]
La suite dans le  second cas s'éloigne de la solution et semble diverger.
\end{frame}

\begin{frame}
\frametitle{La méthode de Gauss-Seidel}

Reprenons la méthode de Jacobi sous une forme développée:
\[
a_{ii} x_i^{(k+1)} = - \sum_{j<i} a_{ij} x_j^{(k)} - \sum_{j>i}a_{ij} x_j^{(k)}+b_i \qquad \forall i \in \llbracket 1, n \rrbracket
\] 
À l'étape $i$, les termes $ \left( x_j^{(k+1)} \right)_{j<i}$ ont été déjà calculés. En général, ils sont plus proches de la solution que $\left( x_j^{(k)}\right)_{j<i}$ dans le cas d'une convergence: d'où l'idée de remplacer $x_j^{(k)}$ par $x_j^{(k+1)}$. Ainsi au lieu d'attendre une itération entière pour corriger chaque composante, la correction se fait au fur et à mesure:
%\begin{empheq}[left=\empheqlbrace]{align*}
%&a_{ij} x_i^{(k+1)} = - \sum_{j<i} a_{ij} x_j^{(k+1)} - \sum_{j>i} a_{ij} x_j^{(k)} +b_i \\
%& x^{(0)} \in \mathbb{R}^n
%\end{empheq}

\[\myredbox{\left\{\begin{array}{l}
a_{ij} x_i^{(k+1)} = - \sum_{j<i} a_{ij} x_j^{(k+1)} - \sum_{j>i} a_{ij} x_j^{(k)} +b_i \\
x^{(0)} \in \mathbb{R}^n

\end{array}\right.}\]


\end{frame}

\begin{frame}
\frametitle{La méthode de Gauss-Seidel}

Cela revient à choisir une méthode itérative avec $P=D-E$ et $N=F$. La méthode de Gauss-Seidel s'écrit matriciellement :
fait au fur et à mesure:
\begin{empheq}[left=\empheqlbrace]{align*}
& (D-E) x^{(k+1)} =F x^{(k)} +b \\
& x^{(0)} \in \mathbb{R}^n
\end{empheq}

ou bien :

\begin{empheq}[left=\empheqlbrace]{align*}
& x^{(k+1)} = B_{GS} x^{k} + c_{GS} \\
& x^{(0)} \in \mathbb{R}^n
\end{empheq}

où $B_{GS}= (D-E)^{-1} \cdot F$ et $c_{GS} = (D-E)^{-1} \cdot b$.


\begin{algo}
\caption{Méthode de Gauss-Seidel}
\For{
$i$ \KwFrom $1$ \KwTo $n$}{
$s=b_i$\;
\For{
$j$ \KwFrom $1$ \KwTo $n$
}{
$s=s-a_{ij} \cdot x_j$
}
$\displaystyle x_i=x_i + \frac{s}{a_{ii}}$
}
\end{algo}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
def GaussSeidel(A,b,x0,N):
    x=x0
    for k in range(N):
        n=len(A)
        y=np.zeros(n)
        for i in range(n):
            x[i]+=(b[i]-A[i]@x)/A[i,i]
    return x

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{La méthode de Gauss-Seidel}
La programmation de la méthode de Gauss-Seidel ne nécessite pas deux tableaux $X$ et $Y$ comme dans la méthode de Jacobi. D'où le gain de stockage. En pratique, la méthode de Gauss-Seidel converge (ou diverge) souvent plus rapidement que celle de Jacobi car on utilise les nouvelles valeurs des composantes dès qu'elles sont calculées.

\begin{theorem}
Pour une matrice tridiagonale :
\[
\rho(B_{GS})=(\rho(B_{J}))^2
\]
Donc la méthode de Gauss-Seidel converge ou diverge plus vite que celle de Jacobi.
\end{theorem}

\begin{block}{proposition}
Si $A$ est une matrice symétrique définie positive, alors la méthode de Gauss-Seidel converge.
\end{block}
\end{frame}

\begin{frame}
\frametitle{La méthode de Gauss-Seidel}
Démo:

$A$ étant symétrique définie positive, on a $a_{ii} = \exp_i^t \cdot A \cdot \exp_i >0$. Donc les termes diagonaux de $D-E$ sont non nuls. D'où $P=D-E$ inversible et $B=(D-E)^{-1}$ a bien un sens.

On a $B=D^{-\frac 12} \cdot \left(I_d - D^{- \frac 12} \cdot E \cdot D^{- \frac 12} \right)^{-1} \cdot D^{- \frac 12} \cdot E^{t}$. 

Posons $L = D^{- \frac 12} \cdot E \cdot D^{- \frac 12}$. On a $B = D^{- \frac 12} \cdot (I_d - L) \cdot L^t \cdot D^{\frac 12} = D^{- \frac 12} \cdot B_1 \cdot D^{\frac 12}$ en posant $B_1=(I-L) \cdot L^t$.

L'équivalence $B \cdot x = \lambda \cdot x \Longleftrightarrow B_1 \left( D^{ \frac 12} \cdot x \right) = \lambda \left( D^{\frac 12}  x \right)$ prouve que $B$ et $B_1$ ont les mêmes valeurs propres.

Soit $\lambda$ une valeur propre de $B_1$ associée à un vecteur propre $x$ tel que $\| x \| =1$. On a $B_1 \cdot x = \lambda x$. Donc $(I-L)^{-1} \cdot L^t \cdot x = \lambda x $. Donc $L^t \cdot x = \lambda (I_d-L) \cdot x$. D'où $\lambda= \frac{x^t \cdot L \cdot x}{1-x^t \cdot L \cdot x}$. Puis en développant l'inégalité $\left(D^{- \frac 12}  \cdot x \right)^t \cdot A \cdot \left( D^{- \frac 12} \cdot x \right)>0 $, on trouve $x^t \cdot L \cdot x < \frac 12$. D'où $| \lambda | <1$. Le rayon spectral de $B$ étant strictement inférieur à $1$, la méthode de Gauss-Seidel converge.



\end{frame}


\begin{frame}
\frametitle{Les méthodes par blocs}

En pratique, les systèmes linéaires obtenus par discrétisation des équations de la physique sont souvent tridiagonales ou tridiagonales par blocs et s'adaptent bien à une résolution par méthode itérative. 

Soit $A \in \mathscr{M}_n(\mathbb{R})$ une matrice inversible. On suppose qu'elle se décompose par blocs :
\[
A=\left(
\begin{BMAT}(c){c.c.c.c}{c.c.c.c}
A_{11} &A_{12} & & \\
A_{21}  & & & \\
&&&\\
&&& A_{nn} 
\end{BMAT} \right)
\]
où $A_{IJ} \in \mathscr{M}_{nI, nJ}(\mathbb{R})$ est une matrice inversible et $(n_1, n_2, \dots, n_p)$ un $p$-uplet tel que $\sum\limits_{I=1}^p n_I=n$.
\end{frame}


\begin{frame}
On décompose la matrice $A$ en une somme de trois matrices par bloc $A=D-E-F$ où :
\[
D=\left(
\begin{BMAT}(c){c.c.c.c}{c.c.c.c}
A_{11} & & & \\
& A_{22} & & \\
&&&\\
&&& A_{nn} 
\end{BMAT} \right) \]
\[
E=\left(
\begin{BMAT}(c){c.c.c.c}{c.c.c.c}
0 & & & \\
-A_{21}  & & & \\
-A_{31} & -A_{32} &&\\
&&& 0
\end{BMAT} \right) \qquad
F=\left(
\begin{BMAT}(c){c.c.c.c}{c.c.c.c}
0 & -A_{12}& & -A_{1n}\\
0 & 0 &-A_{23} & \\
 &  &&\\
&&& 0
\end{BMAT} \right)
\]

\end{frame}

\begin{frame}
Soit $\left(X_1^t,X_2^t,\dots,X_P^t \right)^t $ une décomposition par blocs de $x$ adaptée à la décomposition de la matrice de $A$, c'est-à-dire $ \forall I \in \llbracket 1, P \rrbracket, \;X_I \in \mathbb{R}^{n_I}$.
\[
x=\left(
\begin{BMAT}(c){c}{cccc}
X_1 \\
X_2 \\
\vdots \\
X_P
\end{BMAT} \right) \qquad
b=\left(
\begin{BMAT}(c){c}{cccc}
B_1 \\
B_2 \\
\vdots \\
B_P
\end{BMAT} \right)
\]
La méthode de Jacobi par blocs s'écrit :
\begin{empheq}[left=\empheqlbrace]{align*}
&A_{II} X_I^{(k+1)} = - \sum_{j<i} A_{IJ} X_J^{(k)} -\sum_{j>i} A_{IJ} X_J^{(k)} + B_I \qquad I=1,\dots, P \\
&x^{(0)} \in \mathbb{R}
\end{empheq}
Dans le cas particulier où $P=n$, chaque bloc $A_{IJ}$ se réduit au coefficient $a_{ij}$ et $X_I=x_i$, $B_I=b_i$. On retrouve la méthode de Jacobi par points.
\end{frame}

\begin{frame}
La méthode de relaxation par blocs devient :
\[
A_{ii} \cdot x_i^{(k+1)} = \omega \left( b_i - \sum_{j=1}^{i-1} A_{ij} x_j^{k+1} \right) + \left( 1 - \omega \right) A_{ii} x_i^{(k)} - \omega \sum_{j=i+1}^{p} A_{ij} x_j^{k}
\]
Ce qui est équivalent à :
\[
A_{ii} \left( x_i^{(k+1)} -  x_i^{(k)} \right) = \omega \left( b_i - \sum_{j=1}^{i-1} A_{ij} x_j^{k+1} - \sum_{j=i}^{p} A_{ij} x_j^{k} \right) 
\]


\end{frame}

\begin{frame}
\frametitle{Méthode de relaxation}

La méthode itérative de relaxation généralise les deux méthodes précédentes, Jacobi et Gauss-Seidel.
Elle est définie par la relaxation:
\[\myredbox{
x_i^{(k+1)} = \omega \widetilde{x_i}^{(k+1)} + (1 - \omega) x_i^{(k)}}
\]
où le réel $\omega$ est un paramètre de la méthode et $\widetilde{x_i}^{(k+1)} $ est la composante obtenue à partir de $x_i^{(k)}$ par une méthode de Jacobi ou de Gauss-Seidel.

Si la méthode auxiliaire est celle de Jacobi, on obtient pour $i \in \llbracket 1, n \rrbracket $ :
\[
x_i^{(k+1)} = \frac{\omega}{a_{ii}} \left( b_i - \sum_{\substack{j=i\\j \neq i}}^n a_{ij} x_j^{(k)} \right) + ( 1 - \omega) x_i ^{(k)}
\]
Soit matriciellement :
\[
x^{(k+1)}= \left( I_d - \omega D^{-1} \cdot A \right) \cdot x^{(k)} + \omega D^{-1} \cdot b
\]
Cette méthode est très peu utilisée car en général elle n'apporte aucun gain significatif. 

\end{frame}

\begin{frame}
Si la méthode de base choisie est celle de Gauss-Seidel, la méthode de relaxation est définie par :
\[
x_i^{(k+1)} = \frac{\omega}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)}  - \sum_{j=i+1}^n a_{ij} x_j^{(k)} \right) + ( 1 - \omega) x_i ^{(k)}
\]
Ou bien en retranchant $x_i^{(k)}$ aux deux membres :
\[
\myredbox{x_i^{(k+1)} -x_i^{k}= \frac{\omega}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)}  - \sum_{j=i}^n a_{ij} x_j^{(k)} \right) }
\]
Soit matriciellement :
\[\myredbox{
x^{(k+1)}= \left( \frac{D}{\omega} - E  \right)^{-1} \cdot b +  \left( \frac{D}{\omega} - E  \right)^{-1} \left[ \left( \frac{1}{\omega} -1 \right) \cdot D + F \right] \cdot x^{(k)}
}\]

\end{frame}

\begin{frame}
L'algorithme de la méthode de relaxation est aussi simple que celui de Gauss-Seidel :

\begin{algo}
\caption{Méthode de relaxation}
\For{
$i$ \KwFrom $1$ \KwTo $n$}{
$s=b_i$\;
\For{
$j$ \KwFrom $1$ \KwTo $n$
}{
$s=s-a_{ij} \cdot x_j$
}
$\displaystyle x_i=x_i + \omega \frac{s}{a_{ii}}$
}
\end{algo}
\end{frame}

\begin{frame}
\frametitle{Méthode du gradient}
$A$ matrice symétrique définie positive, $b\in \mathbb{R}^n$ et
\[J(x)=\frac 12(Ax,x)-(b,x)\]
Pour $h\in\mathbb{R}^n$ on a
\[J(x+h)=J(x)+(Ax-b,h)+\frac 12(Ah,h)\]
Donc $\nabla J(x)= Ax-b$ 
\[J(\bar{x})\mbox{ minimal } \Longrightarrow A\bar{x}-b=0\]
Donc la solution de $Ax=b$ réalise le min de $J$:
\[J(\bar{x})=\inf_{x\in\mathbb{R}^n}J(x)\]


\end{frame}

\begin{frame}
\begin{center}
 \begin{tikzpicture}[domain=0:5]
  %\draw[->] (-1,1) -- (4,1)  node[right] {$\scriptstyle  x$};
  %\draw[->] (0,0.5) -- (0,3) node[left] {$\scriptstyle  y$};
   \draw[->,dotted] (0,1) -- (0.5,35/12) node[left] {$\scriptstyle  x_0$};
      \draw[->,dotted] (0,1) -- (2,8/3) node[right] {$\scriptstyle  x_1$};
       \draw[->,dotted] (0,1) --  (3,5/3) node[right] {$\scriptstyle  x_2$};
        \draw (1.25,2.6) node  {$\scriptstyle  h_0$};
        \draw (2.5,1.9) node  {$\scriptstyle  h_1$};
        \draw[blue,->] (0.5,35/12)-- (2,8/3);
         \draw[blue,->] (2,8/3)--  (3,5/3);
          \draw[blue]  (3,5/3)--(3.3,1);
   \path[fill=black]  (0.5,35/12) circle (.3mm) [fill=gray];
     \path[fill=black]  (2,8/3) circle (.3mm) [fill=gray];
     \path[fill=black]  (3,5/3) circle (.3mm) [fill=gray];
\draw [pink,domain=-0.5:3.3,samples=200] plot(\x,{3-(\x-1)^2/3});
  
\end{tikzpicture}
 \end{center}
 
 \[J(x_0)>J(x_1)>J(x_2)>\cdots\]
 On pose $x_1=x_0+h$. On a
\[J(x_1)=J(x_0+h)=J(x_0)+(\nabla J,h)+\frac 12(Ah,h)\]
On choisit $h=-\alpha \nabla J$:
\[\myredbox{x_{k+1}=x_k-\alpha \nabla J(x_k)}\]
\end{frame}

\begin{frame}

\begin{block}{Proposition}
Soit $p\in \mathbb{R}^n$ donné et $f(\alpha)=J(x-\alpha p)$. Alors le minimum de $f$ est réalisé en
\[\alpha=\frac{(\nabla J,p)}{(Ap,p)}\]
\end{block}
Démo:
\[J(x-\alpha p)=J(x)-\alpha(\nabla J,p)+\frac{\alpha^2}{2}(Ap,p)\]
\[\mbox{min }\Longrightarrow \frac{\de}{\de \alpha}J(x-\alpha p)=0\Longrightarrow -(\nabla J,p)+\alpha (Ap,p)=0\]



\end{frame}


\begin{frame}
\frametitle{Méthode du gradient à pas optimal}
On a $\nabla J(x)=Ax-b$ On pose $r=b-Ax$, on a donc
\[\left\{\begin{array}{l}
r_k=b-Ax_k\\
\alpha_k=\frac{\|r_k\|^2}{(Ar_k,r_k)}\\
x_{k+1}=x_k+\alpha_k r_k
\end{array}\right.
\]

\begin{algo}
\caption{Méthode du gradient }
$x_0 \mbox{ donné}$\;
\While{$J(x_k)-J(x_{k+1})>\varepsilon$}{
$r_k=b-Ax_k$\;
$\alpha_k=\frac{\|r_k\|^2}{(Ar_k,r_k)}$\;
$x_{k+1}=x_k+\alpha_k r_k$
}
\end{algo}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
def gradient(A,b,x0,N):
    x=x0.copy()
    for k in range(N):
        r=b-A@x
        alpha=(r@r)/(A@r@r)
        x+=alpha*r
    return x

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Méthode du gradient conjugué}

\begin{algo}
\caption{Méthode du gradient conjugué}
$x_0 \mbox{ donné}$\;
$r_0 =b-A x_0$\;
$p_0=r_0$\;
\For{
$k$ \KwFrom $0$ \KwTo $n-1$}{
$\alpha_k=\frac{\|r_k\|^2}{(Ar_k,r_k)}$\;
$x_{k+1}=x_k+\alpha_k p_k$\;
$r_{k+1}=r_k-\alpha_k A p_k$\;
$\beta_k=\frac{\|r_{k+1}\|^2}{\|r_{k}\|^2}$\;
$p_{k+1}=r_{k+1}+\beta_k  p_k$\;
}
\end{algo}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]

\begin{minted}[
%frame=lines,
framesep=2mm,
baselinestretch=1.2,
%bgcolor=LightGray,
fontsize=\footnotesize,
linenos
]{python}
def GradientConjugue(A,b,x0,N):
    x=x0.copy()
    r=b-A@x
    p=r.copy()
    for k in range(N):
        alpha=(r@r)/(A@r@r)
        x+=alpha*p
        r1=r.copy()
        r+=-alpha*A@p
        beta = (r@r)/(r1@r1)
        p=r+beta*p
        
    return x

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Valeur et vecteur propre}
Soit
\[A=\left(\begin{array}{cc}
10&0\\
-9&1
\end{array}\right)
\]
On fait \[\left\{\begin{array}{l}
x_{k+1}=A x_k\\
x_0\quad \mbox{arbitraire}
\end{array}\right.
\]
\[x_0=\left(\begin{array}{c}
2\\
1
\end{array}\right),\quad
x_1=\left(\begin{array}{c}
20\\
17
\end{array}\right),\quad
x_2=\left(\begin{array}{c}
200\\
-197
\end{array}\right),\quad
x_3=\left(\begin{array}{c}
2000\\
-1997
\end{array}\right)
\]
\[x_4=\left(\begin{array}{c}
20000\\
-19997
\end{array}\right) \cdots
\]
\[\Longrightarrow \left\{\begin{array}{ll}
x_k\quad // \left(\begin{array}{c}
1\\
-1
\end{array}\right) & \mbox{: vecteur propre}\\
x_{k+1}\simeq 10 x_k & \mbox{: valeur propre}
\end{array}\right.
\]
\end{frame}


\begin{frame}
\frametitle{Valeur et vecteur propre}
Si $A$ diagonalisable, 
ici $\lambda_1=10$ associée au vecteur propre $v_1=(1,-1)$ et $\lambda_2=10$ associée à $v_2=(0,1)$.
D'où

\[A^k=\left(\begin{array}{cc}
1&0\\
-1&1
\end{array}\right)
\left(\begin{array}{cc}
10&0\\
0&1
\end{array}\right)^k
\left(\begin{array}{cc}
1&0\\
1&1
\end{array}\right)
= \left(\begin{array}{cc}
10^k&0\\
-10^k+1&1
\end{array}\right)\]
Donc
\[x_k=A^kx_0= \left(\begin{array}{cc}
10^k&0\\
-10^k+1&1
\end{array}\right)\left(\begin{array}{c}
2\\
1
\end{array}\right)=\left(\begin{array}{c}
2\times 10^k\\
-2\times 10^k+3
\end{array}\right)\]
Donc
\[x_k=2\times 10^k\left(\begin{array}{c}
1\\
-1+1.5\times 10^{-k}
\end{array}\right)\simeq 2\times 10^k\left(\begin{array}{c}
1\\
-1
\end{array}\right)
\]
\end{frame}

\begin{frame}

\begin{algo}
\caption{Puissances itérées}
$q_0\in\mathbb{C}^n $ tel que $\|q_0\|=1$\;

\For{
$k$ \KwFrom $1$ \KwTo $N$}{
$x_k=Aq_{k-1}$\;
$\omega_k=\|x_k\|$\;
$\displaystyle q_k= \frac{x_k}{\omega_k}$
}
\end{algo}
\end{frame}
\end{document}





\end{document}

\begin{frame}[fragile]
\begin{lstlisting}
# CAPES externe 2018
def euler(Phi, t0, tf, y0, n):
    h = (tf - t0) / n
    y = y0
    t = t0
    Y = [y0]
    T = [t0]
    for k in range(n):
        y = y + h * Phi(t, y, h)
        t = t + h
        Y.append(y)
        T.append(t)
    return T, Y


\end{lstlisting}

\end{frame}



 
  \end{document}
   

























